{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload updated module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# make screen full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from train import *\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_PL(\n",
    "    n_vocab1 = 13907, n_vocab2 = 0, n_vocab3 = 0, n_label = 5,\n",
    "    emb1_dim = 512, emb2_dim = 0, emb3_dim = 0, hid_dim = 1024,\n",
    "    m_type = 'sy', data_name = 'sy_1', lr = 0.001, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams.lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=10)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    limit_train_batches=50, \n",
    "    limit_val_batches=20,\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=5,\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=5)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# resume from check point (must initiate the model first)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    limit_train_batches=10, \n",
    "    limit_val_batches=3,\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=5,\n",
    "    resume_from_checkpoint='lightning_logs/version_109/checkpoints/epoch=9-step=99.ckpt')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_PL.load_from_checkpoint(\n",
    "    'lightning_logs/version_31/checkpoints/epoch=0-step=19.ckpt')\n",
    "print(model.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574f7f6469e84b25924b8862727eeb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'char f1': 0.9795994153201912,\n",
      " 'char precision': 0.9750617361624487,\n",
      " 'char recall': 0.98417952625897,\n",
      " 'word f1': 0.9485007703551535,\n",
      " 'word precision': 0.9441071456658855,\n",
      " 'word recall': 0.9529354797739252}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# touch all the code to find bugs\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, \n",
    "    fast_dev_run=True, # here\n",
    "    limit_train_batches=1, \n",
    "    limit_val_batches=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# train small number of batchs\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, \n",
    "    limit_train_batches=10, # here\n",
    "    limit_val_batches=1, # here\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# train only 10% of an epoch\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    limit_train_batches=0.1, # here\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# for large batch, run validation every 25% of a training epoch\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    val_check_interval=0.25, # here\n",
    "    limit_val_batches=1,\n",
    "    log_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profile your code to find speed/memory bottlenecks\n",
    "pl.Trainer(profiler=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find LR and BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# search lr\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "dfig = lr_finder.plot(suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set\n",
    "model.hparams.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search batch_size\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "tuner = trainer.tuner.scale_batch_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set\n",
    "model.hparams.batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks: EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_acc',\n",
    "   min_delta=0.00,\n",
    "   patience=3,\n",
    "   verbose=False,\n",
    "   mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=30, \n",
    "    limit_train_batches=5, \n",
    "    limit_val_batches=3,\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    callbacks=[early_stop_callback], # here\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks: LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "def configure_optimizers(self):\n",
    "   optimizer = Adam(...)\n",
    "   scheduler = LambdaLR(optimizer, ...)\n",
    "   return [optimizer], [scheduler]\n",
    "\n",
    "# when scheduler require lr monitor, use callback\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = Trainer(callbacks=[lr_monitor])\n",
    "\n",
    "# The ReduceLROnPlateau scheduler requires a monitor\n",
    "def configure_optimizers(self):\n",
    "   return {\n",
    "       'optimizer': Adam(...),\n",
    "       'lr_scheduler': ReduceLROnPlateau(optimizer, ...),\n",
    "       'monitor': 'metric_to_track'\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_PL(n_vocab1 = 13907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check weight name of the model\n",
    "for weight_name in model.state_dict():\n",
    "    print(weight_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the embedding weight\n",
    "model.x1emb.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe embeding weight before training\n",
    "model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe weight of other layer (to be changed)\n",
    "model.lstm.weight_ih_l0[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight summary shows Non-trainable params\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    limit_train_batches=10, \n",
    "    limit_val_batches=1,\n",
    "    gpus=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight is unchanged after trainning\n",
    "model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight of other layer changes\n",
    "model.lstm.weight_ih_l0[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import weight from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model from checkpoint\n",
    "pretrained_model = BiLSTM_CRF_PL.load_from_checkpoint(\n",
    "    'lightning_logs/version_45/checkpoints/epoch=0-step=9.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at pretrained embedding weight\n",
    "pretrained_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new model\n",
    "new_model = BiLSTM_CRF_PL(n_vocab1 = 13907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore embedding weight (different)\n",
    "new_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the weight of new_model = the weight of pretrained_model\n",
    "new_model.x1emb.weight.data.copy_ = pretrained_model.x1emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding weight has changed\n",
    "new_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    limit_train_batches=10, \n",
    "    limit_val_batches=1,\n",
    "    gpus=1)\n",
    "trainer.fit(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2877, -0.7774, -1.9446,  0.8272,  0.4591], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding weight has changed\n",
    "new_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get glove embedding<br> \n",
    "https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
