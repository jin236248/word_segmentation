{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reload updated module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# make screen full width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from train import *\n",
    "from pytorch_lightning.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_PL(\n",
    "    n_vocab1 = 13907, n_vocab2 = 0, n_vocab3 = 0, n_label = 5,\n",
    "    emb1_dim = 512, emb2_dim = 0, emb3_dim = 0, hid_dim = 1024,\n",
    "    m_type = 'sy', data_name = 'sy_1', lr = 0.001, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams.lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6352cd91eb4b948191c5d4a8667f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13a0115e06dd498683f2d38b548828da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=10)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    limit_train_batches=50, \n",
    "    limit_val_batches=20,\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=5,\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=5)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# resume from check point (must initiate the model first)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10, \n",
    "    limit_train_batches=10, \n",
    "    limit_val_batches=3,\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=5,\n",
    "    resume_from_checkpoint='lightning_logs/version_109/checkpoints/epoch=9-step=99.ckpt')\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load from Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"data_name\":  sy_1\n",
      "\"hid_dim\":   1024\n",
      "\"lr\":           0.001\n",
      "\"m_type\":    sy\n",
      "\"n_label\":    5\n",
      "\"emb1_dim\":    512\n",
      "\"n_vocab1\": 13907\n",
      "\"emb2_dim\":    0\n",
      "\"n_vocab2\": 0\n",
      "\"emb3_dim\":    0\n",
      "\"n_vocab3\": 0\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF_PL.load_from_checkpoint(\n",
    "    'lightning_logs/version_31/checkpoints/epoch=0-step=19.ckpt')\n",
    "print(model.hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574f7f6469e84b25924b8862727eeb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'char f1': 0.9795994153201912,\n",
      " 'char precision': 0.9750617361624487,\n",
      " 'char recall': 0.98417952625897,\n",
      " 'word f1': 0.9485007703551535,\n",
      " 'word precision': 0.9441071456658855,\n",
      " 'word recall': 0.9529354797739252}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = trainer.test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# touch all the code to find bugs\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, \n",
    "    fast_dev_run=True, # here\n",
    "    limit_train_batches=1, \n",
    "    limit_val_batches=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train small number of batchs\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20, \n",
    "    limit_train_batches=10, # here\n",
    "    limit_val_batches=1, # here\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# train only 10% of an epoch\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    limit_train_batches=0.1, # here\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for large batch, run validation every 25% of a training epoch\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=3, \n",
    "    val_check_interval=0.25, # here\n",
    "    limit_val_batches=1,\n",
    "    log_every_n_steps=1,\n",
    "    gpus=1, \n",
    "    weights_summary=None)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Profile your code to find speed/memory bottlenecks\n",
    "pl.Trainer(profiler=\"simple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Find LR and BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# search lr\n",
    "lr_finder = trainer.tuner.lr_find(model)\n",
    "dfig = lr_finder.plot(suggest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set\n",
    "model.hparams.lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 2 succeeded, trying batch size 4\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 4 succeeded, trying batch size 8\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 8 succeeded, trying batch size 16\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 16 succeeded, trying batch size 32\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 32 succeeded, trying batch size 64\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 64 succeeded, trying batch size 128\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 128 succeeded, trying batch size 256\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 256 succeeded, trying batch size 512\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch size 512 succeeded, trying batch size 1024\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 607, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 422, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 575, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 362, in optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1414, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 304, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 311, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 191, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/sgd.py\", line 87, in step\n",
      "    loss = closure()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 569, in train_step_and_backward_closure\n",
      "    result = self.training_step_and_backward(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 660, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 270, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 183, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 153, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 62, in training_step\n",
      "    loss, _, _, _ = self.common_step(batch, True)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 57, in common_step\n",
      "    _, best_sequences, loss = self.forward(x1s, x2s, x3s, xtags, mask, drop)\n",
      "  File \"/home/jin/word_segmentation/model.py\", line 145, in forward\n",
      "    x, self.hidden = self.lstm(x, self.hidden)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1015, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 661, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 6.43 GiB (GPU 0; 10.76 GiB total capacity; 6.64 GiB already allocated; 2.99 GiB free; 6.65 GiB reserved in total by PyTorch)\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 607, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 422, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 575, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 362, in optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1414, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 304, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 311, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 191, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/sgd.py\", line 87, in step\n",
      "    loss = closure()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 569, in train_step_and_backward_closure\n",
      "    result = self.training_step_and_backward(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 660, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 270, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 183, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 153, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 62, in training_step\n",
      "    loss, _, _, _ = self.common_step(batch, True)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 57, in common_step\n",
      "    _, best_sequences, loss = self.forward(x1s, x2s, x3s, xtags, mask, drop)\n",
      "  File \"/home/jin/word_segmentation/model.py\", line 145, in forward\n",
      "    x, self.hidden = self.lstm(x, self.hidden)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1015, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 661, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 8.02 GiB (GPU 0; 10.76 GiB total capacity; 5.51 GiB already allocated; 4.07 GiB free; 5.57 GiB reserved in total by PyTorch)\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 607, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 422, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 575, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 362, in optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1414, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 304, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 311, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 191, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/sgd.py\", line 87, in step\n",
      "    loss = closure()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 569, in train_step_and_backward_closure\n",
      "    result = self.training_step_and_backward(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 660, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 270, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 183, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 153, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 62, in training_step\n",
      "    loss, _, _, _ = self.common_step(batch, True)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 57, in common_step\n",
      "    _, best_sequences, loss = self.forward(x1s, x2s, x3s, xtags, mask, drop)\n",
      "  File \"/home/jin/word_segmentation/model.py\", line 145, in forward\n",
      "    x, self.hidden = self.lstm(x, self.hidden)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1015, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 661, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 5.31 GiB (GPU 0; 10.76 GiB total capacity; 5.53 GiB already allocated; 4.08 GiB free; 5.56 GiB reserved in total by PyTorch)\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 607, in run_train\n",
      "    self.train_loop.run_training_epoch()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 422, in run_training_epoch\n",
      "    batch_output = self.run_training_batch(batch, batch_idx, dataloader_idx)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 575, in run_training_batch\n",
      "    self.optimizer_step(optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 362, in optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1414, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 214, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 134, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 304, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 311, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 191, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/optim/sgd.py\", line 87, in step\n",
      "    loss = closure()\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 569, in train_step_and_backward_closure\n",
      "    result = self.training_step_and_backward(\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 660, in training_step_and_backward\n",
      "    result = self.training_step(split_batch, batch_idx, opt_idx, hiddens)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/trainer/training_loop.py\", line 270, in training_step\n",
      "    training_step_output = self.trainer.accelerator.training_step(args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 183, in training_step\n",
      "    return self.training_type_plugin.training_step(*args)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 153, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 62, in training_step\n",
      "    loss, _, _, _ = self.common_step(batch, True)\n",
      "  File \"/home/jin/word_segmentation/train.py\", line 57, in common_step\n",
      "    _, best_sequences, loss = self.forward(x1s, x2s, x3s, xtags, mask, drop)\n",
      "  File \"/home/jin/word_segmentation/model.py\", line 145, in forward\n",
      "    x, self.hidden = self.lstm(x, self.hidden)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1015, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/jin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py\", line 661, in forward\n",
      "    result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 6.49 GiB (GPU 0; 10.76 GiB total capacity; 6.71 GiB already allocated; 2.90 GiB free; 6.74 GiB reserved in total by PyTorch)\n",
      "Batch size 5000 succeeded, trying batch size 10000\n",
      "Finished batch size finder, will continue with full run using batch size 5000\n",
      "Restored states from the checkpoint file at /home/jin/word_segmentation/scale_batch_size_temp_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# search batch_size\n",
    "trainer = pl.Trainer(gpus=1)\n",
    "tuner = trainer.tuner.scale_batch_size(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set\n",
    "model.hparams.batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Callbacks: EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_acc',\n",
    "   min_delta=0.00,\n",
    "   patience=3,\n",
    "   verbose=False,\n",
    "   mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cf6bc2eced484280e77c1b045d19f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=30, \n",
    "    limit_train_batches=5, \n",
    "    limit_val_batches=3,\n",
    "    log_every_n_steps=1,\n",
    "    flush_logs_every_n_steps=1,\n",
    "    callbacks=[early_stop_callback], # here\n",
    "    gpus=1, \n",
    "    weights_summary=None, \n",
    "    progress_bar_refresh_rate=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Callbacks: LearningRateMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# basic\n",
    "def configure_optimizers(self):\n",
    "   optimizer = Adam(...)\n",
    "   scheduler = LambdaLR(optimizer, ...)\n",
    "   return [optimizer], [scheduler]\n",
    "\n",
    "# when scheduler require lr monitor, use callback\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "trainer = Trainer(callbacks=[lr_monitor])\n",
    "\n",
    "# The ReduceLROnPlateau scheduler requires a monitor\n",
    "def configure_optimizers(self):\n",
    "   return {\n",
    "       'optimizer': Adam(...),\n",
    "       'lr_scheduler': ReduceLROnPlateau(optimizer, ...),\n",
    "       'monitor': 'metric_to_track'\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Freeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF_PL(n_vocab1 = 13907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transitions\n",
      "x1emb.weight\n",
      "lstm.weight_ih_l0\n",
      "lstm.weight_hh_l0\n",
      "lstm.bias_ih_l0\n",
      "lstm.bias_hh_l0\n",
      "lstm.weight_ih_l0_reverse\n",
      "lstm.weight_hh_l0_reverse\n",
      "lstm.bias_ih_l0_reverse\n",
      "lstm.bias_hh_l0_reverse\n",
      "hidden2tag.weight\n",
      "hidden2tag.bias\n"
     ]
    }
   ],
   "source": [
    "# check weight name of the model\n",
    "for weight_name in model.state_dict():\n",
    "    print(weight_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# freeze the embedding weight\n",
    "model.x1emb.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9247,  0.1824, -1.3625,  3.0842,  1.3788])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe embeding weight before training\n",
    "model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0516,  0.0441,  0.0779,  0.0776, -0.0820], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe weight of other layer (to be changed)\n",
    "model.lstm.weight_ih_l0[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | x1emb      | Embedding | 890 K \n",
      "1 | lstm       | LSTM      | 66.6 K\n",
      "2 | hidden2tag | Linear    | 645   \n",
      "3 | dropout    | Dropout   | 0     \n",
      "-----------------------------------------\n",
      "67.2 K    Trainable params\n",
      "890 K     Non-trainable params\n",
      "957 K     Total params\n",
      "3.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f341e269a5f2440babccca93663ba7a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight summary shows Non-trainable params\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    limit_train_batches=10, \n",
    "    limit_val_batches=1,\n",
    "    gpus=1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9247,  0.1824, -1.3625,  3.0842,  1.3788])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight is unchanged after trainning\n",
    "model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0517,  0.0440,  0.0780,  0.0775, -0.0819], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weight of other layer changes\n",
    "model.lstm.weight_ih_l0[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Import weight from pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load pretrained model from checkpoint\n",
    "pretrained_model = BiLSTM_CRF_PL.load_from_checkpoint(\n",
    "    'lightning_logs/version_45/checkpoints/epoch=0-step=9.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3177,  0.5379, -1.0707, -1.2163, -0.9013], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at pretrained embedding weight\n",
    "pretrained_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# define new model\n",
    "new_model = BiLSTM_CRF_PL(n_vocab1 = 13907)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2876, -0.7774, -1.9447,  0.8273,  0.4592], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore embedding weight (different)\n",
    "new_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set the weight of new_model = the weight of pretrained_model\n",
    "new_model.x1emb.weight.data.copy_ = pretrained_model.x1emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2876, -0.7774, -1.9447,  0.8273,  0.4592], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding weight has changed\n",
    "new_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name       | Type      | Params\n",
      "-----------------------------------------\n",
      "0 | x1emb      | Embedding | 890 K \n",
      "1 | lstm       | LSTM      | 66.6 K\n",
      "2 | hidden2tag | Linear    | 645   \n",
      "3 | dropout    | Dropout   | 0     \n",
      "-----------------------------------------\n",
      "957 K     Trainable params\n",
      "0         Non-trainable params\n",
      "957 K     Total params\n",
      "3.829     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 134413 15088 2253\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d7ad0eb6f964d88bab671fc08dd95ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1, \n",
    "    limit_train_batches=10, \n",
    "    limit_val_batches=1,\n",
    "    gpus=1)\n",
    "trainer.fit(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2877, -0.7774, -1.9446,  0.8272,  0.4591], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding weight has changed\n",
    "new_model.x1emb.weight[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "get glove embedding<br> \n",
    "https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('dataset/' + 'sy_1' + '.json') as f: \n",
    "    (train_data, _to_ix, \n",
    "    validate_data, test_data) = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('dataset/' + 'sy_1_small' + '.json', 'w') as f: \n",
    "#     json.dump((train_data[:5000], _to_ix, \n",
    "#                validate_data[:2000], test_data), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(a, b):\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, (2, 3))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(1, show(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from pandas.core.common import flatten\n",
    "for x in flatten([1, [2, 3, [4, 5]]]):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ssg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-e509bf5e400b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcreate_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/word_segmentation/create_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mssg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msyllable_tokenize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ssg'"
     ]
    }
   ],
   "source": [
    "from create_dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
